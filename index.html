<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PerceptualNeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://d2nerf.github.io/img/title_card.png"> -->
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <!-- <meta property="og:url" content="https://github.com/d2nerf/d2nerf"> -->
    <meta property="og:title" content="Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views">
    <meta property="og:description" content="Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using image quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research on how NVS methods perform with respect to perceived video quality. We present the first study on perceptual evaluation of NVS and NeRF variants. For this study, we collected two datasets of scenes captured in a controlled lab environment as well as in-the-wild. In contrast to existing datasets, these scenes come with reference video sequences, allowing us to test for temporal artifacts and subtle distortions that are easily overlooked when viewing only static images. We measured the quality of videos synthesized by several NVS methods in a well-controlled perceptual quality assessment experiment as well as with many existing state-of-the-art image/video quality metrics. We present a detailed analysis of the results and recommendations for dataset and metric selection for NVS evaluation.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views">
    <meta name="twitter:description" content="Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using image quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research on how NVS methods perform with respect to perceived video quality. We present the first study on perceptual evaluation of NVS and NeRF variants. For this study, we collected two datasets of scenes captured in a controlled lab environment as well as in-the-wild. In contrast to existing datasets, these scenes come with reference video sequences, allowing us to test for temporal artifacts and subtle distortions that are easily overlooked when viewing only static images. We measured the quality of videos synthesized by several NVS methods in a well-controlled perceptual quality assessment experiment as well as with many existing state-of-the-art image/video quality metrics. We present a detailed analysis of the results and recommendations for dataset and metric selection for NVS evaluation.">
    <!-- <meta name="twitter:image" content="https://d2nerf.github.io/img/title_card.png"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <!-- <b>D<sup>2</sup>NeRF</b>: Self-Supervised Decoupling of <br>Dynamic and
                Static Objects from a Monocular Video<br> -->
                Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views
                <!-- <small>
                    NeurIPS 2022
                </small> -->
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://scholar.google.com/citations?hl=en&user=XcxDA14AAAAJ">
                                Hanxue Liang
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://chikayan.github.io/">
                                Tianhao Wu
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://paramhanji.github.io/">
                                Param Hanji
                            </a>
                            <br>University of Cambridge
                        </td>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="http://www.banterle.com/francesco/">
                                Francesco Banterle
                            </a>
                            <br>ISTI-CNR
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cst.cam.ac.uk/people/hg470">
                                Hongyun Gao
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cl.cam.ac.uk/~rkm38/">
                                Rafal Mantiuk
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cl.cam.ac.uk/~aco41/">
                                Cengiz Oztireli
                            </a>
                            <br>University of Cambridge
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2303.15206.pdf">
                            <img src="./img/paper_icon.png" height="90px">
                                <h4><strong>Paper (arixv)</strong></h4>
                            </a>
                        </li> 
                        <!-- <li>
                            <a href="">
                            <img src="./img/data_icon.webp" height="90px">
                                <h4><strong>Data</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="">
                            <img src="./img/github_icon.svg" height="90px">
                                <h4><strong>Code</strong></h4>
                                Coming Soon
                            </a>
                        </li>
                        <!-- <li>
                            <a href="">
                            <img src="./img/youtube_icon.png" height="90px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>-->
                    </ul>
                </div>
                
        </div>

        

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="balloonDiv">
                    <video class="video" id="balloon" loop playsinline autoPlay muted src="video/vrig_balloon_wave_crop.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="balloonMerge"></canvas>
                </div>
			</div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2"> -->
                
                <!-- <table width="100%">
                    <div>
                        <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                            <source src="video/circle.mp4" type="video/mp4" />
                        </video>
                    </div>
                </table> -->

            <!-- </div>
        </div> -->


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="waterDiv">
                    <video class="video" id="water" loop playsinline autoPlay muted src="video/vrig_vrig_water_pour.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="waterMerge"></canvas>
                </div>
			</div>
        </div> -->

        <!-- <div class="row" height="400">
            <div class="col-md-8 col-md-offset-2">

                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                        <source src="video/crop.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using image quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research on how NVS methods perform with respect to perceived video quality. We present the first study on perceptual evaluation of NVS and NeRF variants. For this study, we collected two datasets of scenes captured in a controlled lab environment as well as in-the-wild. In contrast to existing datasets, these scenes come with reference video sequences, allowing us to test for temporal artifacts and subtle distortions that are easily overlooked when viewing only static images. We measured the quality of videos synthesized by several NVS methods in a well-controlled perceptual quality assessment experiment as well as with many existing state-of-the-art image/video quality metrics. We present a detailed analysis of the results and recommendations for dataset and metric selection for NVS evaluation.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dataset
                </h3>
                <p class="text-justify">
                    To evaluate NVS methods on video rather than individual views, we collected two new datasets: Lab, captured using a 2D gantry in a laboratory with controlled lighting and background; and Fieldwork, captured in-the-wild, consisting of both indoor and outdoor scenes. Both datasets were captured with Sony A7RIII. Images of selected scenes from both datasets are shown below. The datasets will be made publicly available soon.
                </p>
                <table width="100%">
                    <image src="img/scene_subset.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"></image>
                </table>
            </div>
        </div>

        <!-- <image src="img/scene_subset.png" class="img-responsive" alt="overview" width="70%" style="max-height: 450px;margin:auto;"> -->

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        NVS Results
                    </h3>

                    <div class="text-justify">
                        Examples of reconstructions by various NVS methods on selected scenes from Fieldwork dataset (first two rows) and Lab dataset (third row). Video results and groundtruth datasets will come soon.
                        <br><br>
                        
                    </div>
                    
                    <table width="100%">
                        <image src="img/demo.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"></image>
                    </table>
    
                </div>
            </div>
            
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Perceptual Benchmark Results
                    </h3>

                    <div class="text-justify">
                        We show the perceptual preference for different methods averaged across our collected Lab and Fieldwork datasets, as well as the LLFF dataset. We report both perdataset performance and the overall performance across all three datasets.
                        <br><br>
                        
                    </div>
                    
                    <table width="100%">
                        <image src="img/jod_all_noref.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"></image>
                    </table>
    
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Averaged Bootstrapped Correlations
                    </h3>

                    <div class="text-justify">
                        For each dataset and each NVS method, we averaged subjective JOD scores and quality metric predictions across all scenes. To account for the variance in the data (subjective score variance and scene selection), we estimate the distribution of correlation values using bootstrapping. We show the bootstrapped distributions of correlation coefficients for all metrics computed on (a) Lab, (b) Fieldwork, and (c)LLFF. Spearman Correlation and Pearson Linear Correlation are reported. 
                        <br><br>
                        
                    </div>
                    
                    <table width="100%">
                        <image src="img/spearman_correlations.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"></image>
                    </table>

                    <table width="100%">
                        <image src="img/pearson_correlations.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"></image>
                    </table>
    
                </div>
            </div>
            
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Minimum Difference for Perceptual Improvement
                    </h3>

                    <div class="text-justify">
                        Our analysis lets us determine the smallest improvement in terms of quality metric prediction (e.g., dB for PSNR) that ensures the result is perceptually better; robust to all sources of noise, including the inaccuracy of the metric, variance in the subjective data, and the selection of scenes. We typically want to make the claim of improved performance at &alpha = 0.05 significance level, at which there is only 5&#37 chance that the observed improvement is due to measurement noise or other random factors.
                        <br><br>
                        
                    </div>
                    
                    <table width="100%">
                        <image src="img/met_pred_error_sep.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"></image>
                    </table>
    
                </div>
            </div>
            

            <!-- <div class="row" height="400">
                <div class="col-md-8 col-md-offset-2">
                    <div>
                        <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                            <source src="video/vrig_balloon_wave_crop_compare_test.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">

                <div class="col-md-8 col-md-offset-2">

                    <table width="100%">
                        <tr>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="shark1" loop playsinline autoPlay muted src="video/shark_scam.mp4" onplay="resizeAndPlayDual(this, 'shark2')"></video>
                                    
                                    <canvas height=0 class="videoMerge" id="shark1Merge"></canvas>
                            </td>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="shark2" loop playsinline autoPlay muted src="video/shark_scam_compare_dynamic.mp4"></video>
                                    
                                    <canvas height=0 class="videoMerge" id="shark2Merge"></canvas>
                            </td>
                        </tr>
                    </table>

                </div>
                <div class="video-compare-container-small">
                    <video class="video" id="shark" loop playsinline autoPlay muted src="video/shark_scam.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="sharkMerge"></canvas>
                </div> -->
            <!-- </div> -->
        <!-- </div> -->

        <!-- <div class="row" height="400">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    More results
                </h3>

                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                        <source src="video/pick_drop_compare_all.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-justify">
                    Our method can also remove challenging view-correlated shadows, such as shadows cast by the camera or the photographer:
                    <br><br>
                    
                </div>
                <div class="video-compare-container" id="camShadowDiv">
                    <video class="video" id="camShadow" loop playsinline autoPlay muted src="video/camera_shadow_v2.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="camShadowMerge"></canvas>
                </div>
			</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-justify">
                    <br><br>
                    Our method learns decoupled neural radiance fields, we can therefore render separate components from novel view. As a by-product, we better utilize the network capacity and learn more robust dynamic objects in the scene:  
                    <br><br>
                </div>
                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                        <source src="video/hypernerf_broom-vcam_compare_nv.mp4" type="video/mp4" />
                    </video>
                </div>
                <div class="video-compare-container-small" id="nvDiv">
                    <video class="video" id="nv" loop playsinline autoPlay muted src="video/hypernerf_broom-vcam_compare_nv_hn.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="nvMerge"></canvas>
                </div>
			</div>
        </div>  -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reflection Direction Parameterization
                </h3>
                <div class="text-justify">
                    Previous approaches directly input the camera's view direction into the MLP to predict outgoing radiance. We show that instead using the reflection of the view direction about the normal makes the emittance function significantly easier to learn and interpolate, greatly improving our results.
                    
                    <br><br>
                    
                </div>
                <div class="text-center">
                    <video id="refdir" width="40%" playsinline autoplay loop muted>
                        <source src="video/reflection_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Integrated Directional Encoding
                </h3>
                <div class="text-justify">
                    We explicitly model object roughness using the expected values of a set of spherical harmonics under a von Mises-Fisher distribution whose concentration parameter varies spatially:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/ide.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows sharing the emittance functions between points with different roughnesses. It also enables scene editing after training. Theoretically, our encoding is stationary on the sphere, similar to the Euclidean stationarity of NeRF's positional encoding. <br><br>
                </div>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/ide_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Synthetic Results
                </h3>
                <div class="video-compare-container">
                    <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on Captured Scenes
                </h3>
                Our method also produces accurate renderings and surface normals from captured photographs:
                <div class="video-compare-container" style="width: 100%">
                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                </div>
			</div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Editing
                </h3>
                <div class="text-justify">
                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.
                    <br>
                    We can increase and decrease material roughness:
                </div>

                <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted>
                        <source src="video/pick_drop_compare_all.mp4" type="video/mp4" />
                    </video>
                </div>

                

            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Editing
                </h3>
                <div class="text-justify">
                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.
                    <br>
                    We can increase and decrease material roughness:
                </div>

                <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="margin-top: -5%;">
                        <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />
                    </video>
                </div>

                <div class="text-justify">
                    We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections:
                </div>
                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="v2" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color2.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="v3" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color3.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>

            </div>
        </div> -->

            
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{verbin2021refnerf,
    title={{Ref-NeRF}: Structured View-Dependent Appearance for
           Neural Radiance Fields},
    author={Dor Verbin and Peter Hedman and Ben Mildenhall and
            Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
    journal={CVPR},
    year={2022}
}</textarea>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
