<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PerceptualNeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://d2nerf.github.io/img/title_card.png"> -->
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <!-- <meta property="og:url" content="https://github.com/d2nerf/d2nerf"> -->
    <meta property="og:title" content="Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views">
    <meta property="og:description" content="Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using image quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research on how NVS methods perform with respect to perceived video quality. We present the first study on perceptual evaluation of NVS and NeRF variants. For this study, we collected two datasets of scenes captured in a controlled lab environment as well as in-the-wild. In contrast to existing datasets, these scenes come with reference video sequences, allowing us to test for temporal artifacts and subtle distortions that are easily overlooked when viewing only static images. We measured the quality of videos synthesized by several NVS methods in a well-controlled perceptual quality assessment experiment as well as with many existing state-of-the-art image/video quality metrics. We present a detailed analysis of the results and recommendations for dataset and metric selection for NVS evaluation.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views">
    <meta name="twitter:description" content="Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using image quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research on how NVS methods perform with respect to perceived video quality. We present the first study on perceptual evaluation of NVS and NeRF variants. For this study, we collected two datasets of scenes captured in a controlled lab environment as well as in-the-wild. In contrast to existing datasets, these scenes come with reference video sequences, allowing us to test for temporal artifacts and subtle distortions that are easily overlooked when viewing only static images. We measured the quality of videos synthesized by several NVS methods in a well-controlled perceptual quality assessment experiment as well as with many existing state-of-the-art image/video quality metrics. We present a detailed analysis of the results and recommendations for dataset and metric selection for NVS evaluation.">
    <!-- <meta name="twitter:image" content="https://d2nerf.github.io/img/title_card.png"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <!-- <b>D<sup>2</sup>NeRF</b>: Self-Supervised Decoupling of <br>Dynamic and
                Static Objects from a Monocular Video<br> -->
                Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views
                <!-- <small>
                    NeurIPS 2022
                </small> -->
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://scholar.google.com/citations?hl=en&user=XcxDA14AAAAJ">
                                Hanxue Liang
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://chikayan.github.io/">
                                Tianhao Wu
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://paramhanji.github.io/">
                                Param Hanji
                            </a>
                            <br>University of Cambridge
                        </td>
                        </td>
                    </tr>
                </table>
            </div>
        </div>
        <div class="row" id="author-roww" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="http://www.banterle.com/francesco/">
                                Francesco Banterle
                            </a>
                            <br>ISTI-CNR
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cst.cam.ac.uk/people/hg470">
                                Hongyun Gao
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cl.cam.ac.uk/~rkm38/">
                                Rafal Mantiuk
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cl.cam.ac.uk/~aco41/">
                                Cengiz Oztireli
                            </a>
                            <br>University of Cambridge
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2303.15206.pdf">
                            <img src="./img/paper_icon.png" height="90px">
                                <h4><strong>Paper (arixv)</strong></h4>
                            </a>
                        </li> 
                        <li>
                            <a href="">
                            <img src="./img/github_icon.svg" height="90px">
                                <h4><strong>Code</strong></h4>
                                Coming Soon
                            </a>
                        </li>
                    </ul>
                </div>
                
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using image quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research on how NVS methods perform with respect to perceived video quality. We present the first study on perceptual evaluation of NVS and NeRF variants. For this study, we collected two datasets of scenes captured in a controlled lab environment as well as in-the-wild. In contrast to existing datasets, these scenes come with reference video sequences, allowing us to test for temporal artifacts and subtle distortions that are easily overlooked when viewing only static images. We measured the quality of videos synthesized by several NVS methods in a well-controlled perceptual quality assessment experiment as well as with many existing state-of-the-art image/video quality metrics. We present a detailed analysis of the results and recommendations for dataset and metric selection for NVS evaluation.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dataset
                </h3>
                <p class="text-justify">
                    To evaluate NVS methods on video rather than individual views, we collected two new datasets: Lab, captured using a 2D gantry in a laboratory with controlled lighting and background; and Fieldwork, captured in-the-wild, consisting of both indoor and outdoor scenes. Both datasets were captured with Sony A7RIII. Images of selected scenes from both datasets are shown below. The datasets will be made publicly available soon.
                </p>
                <!-- <table width="100%">
                    <image src="img/scene_subset.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"></image>
                </table> -->
                <p class="text-justify">
                    Fieldwork Dataset
                </p>
                <table width="100%">
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=dinosaur,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Dinosaur</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=elephant,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Elephant</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=whale,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Whale</figcaption>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=bears,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Bears</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=giraffe,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Giraffe</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=geopards,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Leopards</figcaption>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=vespa,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Vespa</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=statue-1,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Puccini statue</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=statue-2,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Naiad statue</figcaption>
                        </td>
                    </tr>
                </table>
                <p class="text-justify">
                    Lab Dataset
                </p>
                <table width="100%">
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=dinosaur,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Dinosaur</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=elephant,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Elephant</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=whale,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Whale</figcaption>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=bears,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Bears</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=giraffe,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Giraffe</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=geopards,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Leopards</figcaption>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=vespa,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Vespa</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=statue-1,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Puccini statue</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=statue-2,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Naiad statue</figcaption>
                        </td>
                    </tr>
                </table>
                <!-- <style>
                    figcaption {
                      /* position: absolute;
                      bottom: 0;
                      left: 0;
                      right: 0; */
                      text-align: center;
                      /* padding: 10px; */
                    }
                </style> -->
                <!-- <table width="100%">
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=vespa,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Vespa</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=statue-1,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Puccini statue</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=statue-2,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Naiad statue</figcaption>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=car-fig,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Toys</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=metal,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Metal</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=Glass,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Glass</figcaption>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=farm,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>Glossy animals</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=woods,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>CD-occlusions</figcaption>
                        </td>
                        <td>
                            <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px; clip-path: inset(1px 1px);">
                                <source src="video/scene=woods,method=reference.mp4" type="video/mp4" />
                            </video>
                            <figcaption>CD-occlusions</figcaption>
                        </td>
                    </tr>
                </table> -->
                <style>
                    figcaption {
                      /* position: absolute;
                      bottom: 0;
                      left: 0;
                      right: 0; */
                      text-align: center;
                      /* padding: 10px; */
                    }
                </style>
            </div>
        </div>


            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        NVS Results
                    </h3>
                    <p class="text-justify">
                        Examples of reconstructions by various NVS methods on selected scenes from Fieldwork dataset (first two rows) and Lab dataset (third row). Video results and groundtruth datasets will come soon.
                    </p>
                    <table width="100%">
                        <image src="img/demo.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"></image>
                    </table>
    
                </div>
            </div>
            
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Perceptual Benchmark Results
                    </h3>
                    <p class="text-justify">
                        We show the perceptual preference for different methods averaged across our collected Lab and Fieldwork datasets, as well as the LLFF dataset. We report both perdataset performance and the overall performance across all three datasets.
                    </p>
                    <table width="100%">
                        <image src="img/jod_all_noref.png" class="img-responsive" alt="overview" width="70%" style="margin:auto;"></image>
                    </table>
    
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Averaged Bootstrapped Correlations
                    </h3>

                    <div class="text-justify">
                        For each dataset and each NVS method, we averaged subjective JOD scores and quality metric predictions across all scenes. To account for the variance in the data (subjective score variance and scene selection), we estimate the distribution of correlation values using bootstrapping. We show the bootstrapped distributions of correlation coefficients for all metrics computed on (a) Lab, (b) Fieldwork, and (c)LLFF. 
                        <br><br>
                        
                    </div>

                    <div class="text-justify">
                        Bootstrapped Spearman Correlation
                        <br>
                    </div>

                    <table width="100%">
                        <image src="img/spearman_correlations.png" class="img-responsive" alt="overview" width="80%" style="margin:auto;"></image>
                    </table>

                    <div class="text-justify">
                        Bootstrapped Pearson Linear Correlation
                        <br>
                    </div>

                    <table width="100%">
                        <image src="img/pearson_correlations.png" class="img-responsive" alt="overview" width="80%" style="margin:auto;"></image>
                    </table>
    
                </div>
            </div>
            
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Minimum Difference for Perceptual Improvement
                    </h3>

                    <div class="text-justify">
                        Our analysis lets us determine the smallest improvement in terms of quality metric prediction (e.g., dB for PSNR) that ensures the result is perceptually better; robust to all sources of noise, including the inaccuracy of the metric, variance in the subjective data, and the selection of scenes. We typically want to make the claim of improved performance at &alpha = 0.05 significance level, at which there is only 5&#37 chance that the observed improvement is due to measurement noise or other random factors.
                        <br>
                    </div>
                    
                    <table width="100%">
                        <image src="img/met_pred_error_sep.png" class="img-responsive" alt="overview" width="70%" style="margin:auto;"></image>
                    </table>
    
                </div>
            </div>
            

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
